{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Computer Vision: Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer Science: COMS W 4995 011"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposal: Due November 7, 2023\n",
    "### Presentations: Due December 5 and 7, 2023\n",
    "### Final Report: Due December 8, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final project is one of the most important and, hopefully, exciting components of the course. You will have the opportunity to develop a deep learning system of your own choosing. \n",
    "You are free to select whatever framework (Pytorch, Tensorflow, etc.) you like, but you need create a report on your project in a Jupyter notebook. You are also free build on publically available models and code, but your report must clearly give attribution for the work of others and must clearly delineate your contributions. Also, half of the class will present their project during the last 2 days. All of the class will prepare videos of their presentation and submit these when the final report is due. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project description should include the title of the project, participants, a description of the objectives of the project, and a plan for how the project will be completed. The description of the objectives should include modest predictions of the success of the project. The plan for completion should include a description of the training data and how it will be obtained, a discussion of what deep learning framework will be used and why, and a rough description of the planned network architecture.\n",
    "\n",
    "You are permitted to work together on a project in groups of two or three, but group size must not exceed three participants.  For group projects there must be a clearly delineated division of labor: you should state in the project description and project report who was responsible for which portion of the project. Each student must hand in a separate report. (Students will not necessarily get the same grade for the same project.)\n",
    "\n",
    "You should mention whether you are simply re-implementing what others have done before but applying to new data or whether you are attempting to do something new to the best of your knowledge. Creative and original projects will be judged more kindly than those that are rehashing something in the existing literature. And projects that include a component in which data is acquired/curated into training and validation sets will be veiwed more favorably than those that simply download an existing data set such as CIFAR-100.\n",
    "\n",
    "As this is a computer vision course it is expected that your data will be visual, but exceptions might be made if the student is enthusiastic and persuasive enough. The most straightforward project would be to build a system that classifies images into categories. A more difficult project might be to build a system that detects and localizes a type of object within an image. A still more complicated project might involve joining a ConvNet/Vision Transformer with an LSTM/Transformer for a problem (like image captioning) that requires vision and language. But again, creative and original projects will be judged more kindly.  \n",
    "\n",
    "It is important to scope your project so that you get some working results. Project reports that say \"I tried this and this but nothing seemed to work...\" are discouraged. Above all, you should demonstrate end-to-end fluency in the basics of deep learning. \n",
    "\n",
    "I cannot wait to see the results. Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### participants\n",
    "Jordyn Kim (jk4671), Sun Kim (syk2145)\n",
    "\n",
    "##### Project Objectives\n",
    "\n",
    "The objective of this project is to develop a deep learning model to separate single-channel cinematic audio into three distinct components: voice, background music, and sound effects. This task, often referred to as the cocktail fork separation problem, is highly applicable in the entertainment industry and a prominent focus within the field of audio signal processing.\n",
    "\n",
    "Based on our preliminary research, we expect voice separation to yield promising results, with clear and high-quality separation. Achieving precise separation of music and effects may, however, face some constraints due to (1) the computational overhead of our chosen architecture (RNN-based models), (2) limited available computational resources, and (3) additional time required for preprocessing audio signals into spectrograms.\n",
    "\n",
    "##### Project Plan\n",
    "\n",
    "The project will be completed in three main phases:\n",
    "1.\tInitial Setup and Data Preparation (Phase 1)\n",
    "\t- This phase includes setting up the necessary infrastructure, including establishing a Git repository, importing the Divide and Remaster (DnR) dataset, and converting audio data into spectrograms or mel-spectrograms using short-time Fourier transform (STFT).\n",
    "2.\tVoice vs. Non-Voice Separation (Phase 2)\n",
    "\t-\tThe focus of this phase is to isolate voice from background music and sound effects. We plan to test two different approaches:\n",
    "\t-\tImplementing Band-Split RNN (BSRNN): This model, based on this paper https://arxiv.org/abs/2209.15174, employs the loss function combines frequency-domain and time-domain L1 losses and Adam optimizer for training.\n",
    "\t-\tFine-Tuning Pre-trained Models: We also aim to experiment with fine-tuning models pre-trained on musical source separation, Open-Unmix, and Demucs. As noted in this paper https://arxiv.org/pdf/2308.06981, models trained on music-only data tend to perform better in separating voice from non-voice. We can leverage pre-trained models available through the MUSDB18 music database.\n",
    "3.\tMusic vs. Sound Effects Differentiation (Phase 3)\n",
    "\t-\tIn this final phase, we will build on the voice separation results by extending the band-split RNN model to further differentiate background music from sound effects. Similar network structures will be applied, and additional features or model variations may be introduced based on the results from Phase 2.\n",
    "\n",
    "##### Training Data\n",
    "\n",
    "We will use the Divide and Remaster (DnR) dataset https://zenodo.org/records/6949108, approximately 200GB in size, which is specifically structured for the cocktail fork separation problem. This dataset includes labeled annotations for music, sound effects, and speech to support structured training.\n",
    "\n",
    "##### Deep Learning Framework\n",
    "\n",
    "We plan to use PyTorch for model implementation due to its compatibility with pre-trained models like Demucs and Open-Unmix, as well as its flexibility for custom network structures.\n",
    "\n",
    "##### Network Architecture\n",
    "\n",
    "Our network architecture will primarily focus on the Band-Split RNN (BSRNN) model. The initial step in the pipeline involves splitting spectrogram data into sub-bands by frequency, where each band is then normalized and processed through a fully connected dense layer. The second step introduces two RNNs: one handling the frequency domain and the other the time domain using bidirectional LSTMs. The outputs from all bands are then merged to form the final representation.\n",
    "\n",
    "In case the BSRNN implementation proves too complex, we will experiment with alternative pre-trained models, such as Open-Unmix and Demucs, to address the separation task.\n",
    "\n",
    "##### Team Responsibilities\n",
    "\n",
    "-\tData Collection and Preprocessing\t\n",
    "-\tModel Implementation\n",
    "-\tFine-Tuning and Integration of Pre-Trained Models\t\n",
    "-\tEvaluation and Optimization\n",
    "\n",
    "##### Evaluation\n",
    "\n",
    "We will evaluate model performance using the Signal-to-Noise-and-Distortion Ratio (SDR) metric, a standard measure for assessing audio source separation quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Presentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To allow students to present their work in two class periods, each student will have only 3 minutes, not a second more. We will be strict about the timing, so you should practice your presentation. The key here is to get across three things: what you did, how you did it, and how well it worked. Students working in groups of two will get 6 minutes and groups of three will get 9 minutes. Note only half of the class will present during the last two days of class, but all of the groups will submit videos of their project. The time limit rules for the videos is the same as for the presentations. And the video can simply be a narration over a slideshow, but again each student in a group needs to present/narrate the work they did.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The report should be done as a Jupyter Notebook. The report should be a complete description of the objectives of the work, the methods used to solve the problem, experimental evidence of a working system, the code, and clear delineation of what you have done vs. what you are leveraging that others have done. If you have used the work of others YOU MUST INCLUDE ATTRIBUTION by citing this work inline and as part of a \"bibliography\" at the end. You should describe what worked, what did not, and why. If you are working in a group you need to submit your own report and this report should be clear about what your individual contribution was. It is ok to include your collaborators work in your report, but you must be clear about your section and write this yourself. This project report constitutes a large fraction of your final grade--take it seriously and include enough material and details for us to give you a good grade. If you are having trouble imagining the structure of the report, refer to published research papers in CV as a possible model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "\n",
    "from demucs import pretrained\n",
    "# from demucs.pretrained import get_model\n",
    "from demucs.apply import apply_model\n",
    "\n",
    "# Add the source directory to the Python path\n",
    "source_path = os.path.abspath(\"./source\")\n",
    "if source_path not in sys.path:\n",
    "    sys.path.append(source_path)\n",
    "import importlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HTDemucs(\n",
       "  (encoder): ModuleList(\n",
       "    (0): HEncLayer(\n",
       "      (conv): Conv2d(4, 48, kernel_size=(8, 1), stride=(4, 1), padding=(2, 0))\n",
       "      (norm1): Identity()\n",
       "      (rewrite): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (norm2): Identity()\n",
       "      (dconv): DConv(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv1d(48, 6, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): GroupNorm(1, 6, eps=1e-05, affine=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Conv1d(6, 96, kernel_size=(1,), stride=(1,))\n",
       "            (4): GroupNorm(1, 96, eps=1e-05, affine=True)\n",
       "            (5): GLU(dim=1)\n",
       "            (6): LayerScale()\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv1d(48, 6, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "            (1): GroupNorm(1, 6, eps=1e-05, affine=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Conv1d(6, 96, kernel_size=(1,), stride=(1,))\n",
       "            (4): GroupNorm(1, 96, eps=1e-05, affine=True)\n",
       "            (5): GLU(dim=1)\n",
       "            (6): LayerScale()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): HEncLayer(\n",
       "      (conv): Conv2d(48, 96, kernel_size=(8, 1), stride=(4, 1), padding=(2, 0))\n",
       "      (norm1): Identity()\n",
       "      (rewrite): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (norm2): Identity()\n",
       "      (dconv): DConv(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv1d(96, 12, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): GroupNorm(1, 12, eps=1e-05, affine=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Conv1d(12, 192, kernel_size=(1,), stride=(1,))\n",
       "            (4): GroupNorm(1, 192, eps=1e-05, affine=True)\n",
       "            (5): GLU(dim=1)\n",
       "            (6): LayerScale()\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv1d(96, 12, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "            (1): GroupNorm(1, 12, eps=1e-05, affine=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Conv1d(12, 192, kernel_size=(1,), stride=(1,))\n",
       "            (4): GroupNorm(1, 192, eps=1e-05, affine=True)\n",
       "            (5): GLU(dim=1)\n",
       "            (6): LayerScale()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): HEncLayer(\n",
       "      (conv): Conv2d(96, 192, kernel_size=(8, 1), stride=(4, 1), padding=(2, 0))\n",
       "      (norm1): Identity()\n",
       "      (rewrite): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (norm2): Identity()\n",
       "      (dconv): DConv(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv1d(192, 24, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): GroupNorm(1, 24, eps=1e-05, affine=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Conv1d(24, 384, kernel_size=(1,), stride=(1,))\n",
       "            (4): GroupNorm(1, 384, eps=1e-05, affine=True)\n",
       "            (5): GLU(dim=1)\n",
       "            (6): LayerScale()\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv1d(192, 24, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "            (1): GroupNorm(1, 24, eps=1e-05, affine=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Conv1d(24, 384, kernel_size=(1,), stride=(1,))\n",
       "            (4): GroupNorm(1, 384, eps=1e-05, affine=True)\n",
       "            (5): GLU(dim=1)\n",
       "            (6): LayerScale()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): HEncLayer(\n",
       "      (conv): Conv2d(192, 384, kernel_size=(8, 1), stride=(4, 1), padding=(2, 0))\n",
       "      (norm1): Identity()\n",
       "      (rewrite): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (norm2): Identity()\n",
       "      (dconv): DConv(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv1d(384, 48, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): GroupNorm(1, 48, eps=1e-05, affine=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Conv1d(48, 768, kernel_size=(1,), stride=(1,))\n",
       "            (4): GroupNorm(1, 768, eps=1e-05, affine=True)\n",
       "            (5): GLU(dim=1)\n",
       "            (6): LayerScale()\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv1d(384, 48, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "            (1): GroupNorm(1, 48, eps=1e-05, affine=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Conv1d(48, 768, kernel_size=(1,), stride=(1,))\n",
       "            (4): GroupNorm(1, 768, eps=1e-05, affine=True)\n",
       "            (5): GLU(dim=1)\n",
       "            (6): LayerScale()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): ModuleList(\n",
       "    (0): HDecLayer(\n",
       "      (conv_tr): ConvTranspose2d(384, 192, kernel_size=(8, 1), stride=(4, 1))\n",
       "      (norm2): Identity()\n",
       "      (rewrite): Conv2d(384, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (norm1): Identity()\n",
       "      (dconv): DConv(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv1d(384, 48, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): GroupNorm(1, 48, eps=1e-05, affine=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Conv1d(48, 768, kernel_size=(1,), stride=(1,))\n",
       "            (4): GroupNorm(1, 768, eps=1e-05, affine=True)\n",
       "            (5): GLU(dim=1)\n",
       "            (6): LayerScale()\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv1d(384, 48, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "            (1): GroupNorm(1, 48, eps=1e-05, affine=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Conv1d(48, 768, kernel_size=(1,), stride=(1,))\n",
       "            (4): GroupNorm(1, 768, eps=1e-05, affine=True)\n",
       "            (5): GLU(dim=1)\n",
       "            (6): LayerScale()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): HDecLayer(\n",
       "      (conv_tr): ConvTranspose2d(192, 96, kernel_size=(8, 1), stride=(4, 1))\n",
       "      (norm2): Identity()\n",
       "      (rewrite): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (norm1): Identity()\n",
       "      (dconv): DConv(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv1d(192, 24, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): GroupNorm(1, 24, eps=1e-05, affine=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Conv1d(24, 384, kernel_size=(1,), stride=(1,))\n",
       "            (4): GroupNorm(1, 384, eps=1e-05, affine=True)\n",
       "            (5): GLU(dim=1)\n",
       "            (6): LayerScale()\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv1d(192, 24, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "            (1): GroupNorm(1, 24, eps=1e-05, affine=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Conv1d(24, 384, kernel_size=(1,), stride=(1,))\n",
       "            (4): GroupNorm(1, 384, eps=1e-05, affine=True)\n",
       "            (5): GLU(dim=1)\n",
       "            (6): LayerScale()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): HDecLayer(\n",
       "      (conv_tr): ConvTranspose2d(96, 48, kernel_size=(8, 1), stride=(4, 1))\n",
       "      (norm2): Identity()\n",
       "      (rewrite): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (norm1): Identity()\n",
       "      (dconv): DConv(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv1d(96, 12, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): GroupNorm(1, 12, eps=1e-05, affine=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Conv1d(12, 192, kernel_size=(1,), stride=(1,))\n",
       "            (4): GroupNorm(1, 192, eps=1e-05, affine=True)\n",
       "            (5): GLU(dim=1)\n",
       "            (6): LayerScale()\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv1d(96, 12, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "            (1): GroupNorm(1, 12, eps=1e-05, affine=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Conv1d(12, 192, kernel_size=(1,), stride=(1,))\n",
       "            (4): GroupNorm(1, 192, eps=1e-05, affine=True)\n",
       "            (5): GLU(dim=1)\n",
       "            (6): LayerScale()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): HDecLayer(\n",
       "      (conv_tr): ConvTranspose2d(48, 12, kernel_size=(8, 1), stride=(4, 1))\n",
       "      (norm2): Identity()\n",
       "      (rewrite): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (norm1): Identity()\n",
       "      (dconv): DConv(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv1d(48, 6, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): GroupNorm(1, 6, eps=1e-05, affine=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Conv1d(6, 96, kernel_size=(1,), stride=(1,))\n",
       "            (4): GroupNorm(1, 96, eps=1e-05, affine=True)\n",
       "            (5): GLU(dim=1)\n",
       "            (6): LayerScale()\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv1d(48, 6, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "            (1): GroupNorm(1, 6, eps=1e-05, affine=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Conv1d(6, 96, kernel_size=(1,), stride=(1,))\n",
       "            (4): GroupNorm(1, 96, eps=1e-05, affine=True)\n",
       "            (5): GLU(dim=1)\n",
       "            (6): LayerScale()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (tencoder): ModuleList(\n",
       "    (0): HEncLayer(\n",
       "      (conv): Conv1d(2, 48, kernel_size=(8,), stride=(4,), padding=(2,))\n",
       "      (norm1): Identity()\n",
       "      (rewrite): Conv1d(48, 96, kernel_size=(1,), stride=(1,))\n",
       "      (norm2): Identity()\n",
       "      (dconv): DConv(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv1d(48, 6, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): GroupNorm(1, 6, eps=1e-05, affine=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Conv1d(6, 96, kernel_size=(1,), stride=(1,))\n",
       "            (4): GroupNorm(1, 96, eps=1e-05, affine=True)\n",
       "            (5): GLU(dim=1)\n",
       "            (6): LayerScale()\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv1d(48, 6, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "            (1): GroupNorm(1, 6, eps=1e-05, affine=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Conv1d(6, 96, kernel_size=(1,), stride=(1,))\n",
       "            (4): GroupNorm(1, 96, eps=1e-05, affine=True)\n",
       "            (5): GLU(dim=1)\n",
       "            (6): LayerScale()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): HEncLayer(\n",
       "      (conv): Conv1d(48, 96, kernel_size=(8,), stride=(4,), padding=(2,))\n",
       "      (norm1): Identity()\n",
       "      (rewrite): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
       "      (norm2): Identity()\n",
       "      (dconv): DConv(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv1d(96, 12, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): GroupNorm(1, 12, eps=1e-05, affine=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Conv1d(12, 192, kernel_size=(1,), stride=(1,))\n",
       "            (4): GroupNorm(1, 192, eps=1e-05, affine=True)\n",
       "            (5): GLU(dim=1)\n",
       "            (6): LayerScale()\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv1d(96, 12, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "            (1): GroupNorm(1, 12, eps=1e-05, affine=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Conv1d(12, 192, kernel_size=(1,), stride=(1,))\n",
       "            (4): GroupNorm(1, 192, eps=1e-05, affine=True)\n",
       "            (5): GLU(dim=1)\n",
       "            (6): LayerScale()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): HEncLayer(\n",
       "      (conv): Conv1d(96, 192, kernel_size=(8,), stride=(4,), padding=(2,))\n",
       "      (norm1): Identity()\n",
       "      (rewrite): Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
       "      (norm2): Identity()\n",
       "      (dconv): DConv(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv1d(192, 24, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): GroupNorm(1, 24, eps=1e-05, affine=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Conv1d(24, 384, kernel_size=(1,), stride=(1,))\n",
       "            (4): GroupNorm(1, 384, eps=1e-05, affine=True)\n",
       "            (5): GLU(dim=1)\n",
       "            (6): LayerScale()\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv1d(192, 24, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "            (1): GroupNorm(1, 24, eps=1e-05, affine=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Conv1d(24, 384, kernel_size=(1,), stride=(1,))\n",
       "            (4): GroupNorm(1, 384, eps=1e-05, affine=True)\n",
       "            (5): GLU(dim=1)\n",
       "            (6): LayerScale()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): HEncLayer(\n",
       "      (conv): Conv1d(192, 384, kernel_size=(8,), stride=(4,), padding=(2,))\n",
       "      (norm1): Identity()\n",
       "      (rewrite): Conv1d(384, 768, kernel_size=(1,), stride=(1,))\n",
       "      (norm2): Identity()\n",
       "      (dconv): DConv(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv1d(384, 48, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): GroupNorm(1, 48, eps=1e-05, affine=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Conv1d(48, 768, kernel_size=(1,), stride=(1,))\n",
       "            (4): GroupNorm(1, 768, eps=1e-05, affine=True)\n",
       "            (5): GLU(dim=1)\n",
       "            (6): LayerScale()\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv1d(384, 48, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "            (1): GroupNorm(1, 48, eps=1e-05, affine=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Conv1d(48, 768, kernel_size=(1,), stride=(1,))\n",
       "            (4): GroupNorm(1, 768, eps=1e-05, affine=True)\n",
       "            (5): GLU(dim=1)\n",
       "            (6): LayerScale()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (tdecoder): ModuleList(\n",
       "    (0): HDecLayer(\n",
       "      (conv_tr): ConvTranspose1d(384, 192, kernel_size=(8,), stride=(4,))\n",
       "      (norm2): Identity()\n",
       "      (rewrite): Conv1d(384, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (norm1): Identity()\n",
       "      (dconv): DConv(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv1d(384, 48, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): GroupNorm(1, 48, eps=1e-05, affine=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Conv1d(48, 768, kernel_size=(1,), stride=(1,))\n",
       "            (4): GroupNorm(1, 768, eps=1e-05, affine=True)\n",
       "            (5): GLU(dim=1)\n",
       "            (6): LayerScale()\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv1d(384, 48, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "            (1): GroupNorm(1, 48, eps=1e-05, affine=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Conv1d(48, 768, kernel_size=(1,), stride=(1,))\n",
       "            (4): GroupNorm(1, 768, eps=1e-05, affine=True)\n",
       "            (5): GLU(dim=1)\n",
       "            (6): LayerScale()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): HDecLayer(\n",
       "      (conv_tr): ConvTranspose1d(192, 96, kernel_size=(8,), stride=(4,))\n",
       "      (norm2): Identity()\n",
       "      (rewrite): Conv1d(192, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (norm1): Identity()\n",
       "      (dconv): DConv(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv1d(192, 24, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): GroupNorm(1, 24, eps=1e-05, affine=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Conv1d(24, 384, kernel_size=(1,), stride=(1,))\n",
       "            (4): GroupNorm(1, 384, eps=1e-05, affine=True)\n",
       "            (5): GLU(dim=1)\n",
       "            (6): LayerScale()\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv1d(192, 24, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "            (1): GroupNorm(1, 24, eps=1e-05, affine=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Conv1d(24, 384, kernel_size=(1,), stride=(1,))\n",
       "            (4): GroupNorm(1, 384, eps=1e-05, affine=True)\n",
       "            (5): GLU(dim=1)\n",
       "            (6): LayerScale()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): HDecLayer(\n",
       "      (conv_tr): ConvTranspose1d(96, 48, kernel_size=(8,), stride=(4,))\n",
       "      (norm2): Identity()\n",
       "      (rewrite): Conv1d(96, 192, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (norm1): Identity()\n",
       "      (dconv): DConv(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv1d(96, 12, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): GroupNorm(1, 12, eps=1e-05, affine=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Conv1d(12, 192, kernel_size=(1,), stride=(1,))\n",
       "            (4): GroupNorm(1, 192, eps=1e-05, affine=True)\n",
       "            (5): GLU(dim=1)\n",
       "            (6): LayerScale()\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv1d(96, 12, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "            (1): GroupNorm(1, 12, eps=1e-05, affine=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Conv1d(12, 192, kernel_size=(1,), stride=(1,))\n",
       "            (4): GroupNorm(1, 192, eps=1e-05, affine=True)\n",
       "            (5): GLU(dim=1)\n",
       "            (6): LayerScale()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): HDecLayer(\n",
       "      (conv_tr): ConvTranspose1d(48, 6, kernel_size=(8,), stride=(4,))\n",
       "      (norm2): Identity()\n",
       "      (rewrite): Conv1d(48, 96, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (norm1): Identity()\n",
       "      (dconv): DConv(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv1d(48, 6, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): GroupNorm(1, 6, eps=1e-05, affine=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Conv1d(6, 96, kernel_size=(1,), stride=(1,))\n",
       "            (4): GroupNorm(1, 96, eps=1e-05, affine=True)\n",
       "            (5): GLU(dim=1)\n",
       "            (6): LayerScale()\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv1d(48, 6, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "            (1): GroupNorm(1, 6, eps=1e-05, affine=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Conv1d(6, 96, kernel_size=(1,), stride=(1,))\n",
       "            (4): GroupNorm(1, 96, eps=1e-05, affine=True)\n",
       "            (5): GLU(dim=1)\n",
       "            (6): LayerScale()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (freq_emb): ScaledEmbedding(\n",
       "    (embedding): Embedding(512, 48)\n",
       "  )\n",
       "  (channel_upsampler): Conv1d(384, 512, kernel_size=(1,), stride=(1,))\n",
       "  (channel_downsampler): Conv1d(512, 384, kernel_size=(1,), stride=(1,))\n",
       "  (channel_upsampler_t): Conv1d(384, 512, kernel_size=(1,), stride=(1,))\n",
       "  (channel_downsampler_t): Conv1d(512, 384, kernel_size=(1,), stride=(1,))\n",
       "  (crosstransformer): CrossTransformerEncoder(\n",
       "    (norm_in): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm_in_t): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (layers): ModuleList(\n",
       "      (0): MyTransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.02, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.02, inplace=False)\n",
       "        (dropout2): Dropout(p=0.02, inplace=False)\n",
       "        (norm_out): MyGroupNorm(1, 512, eps=1e-05, affine=True)\n",
       "        (gamma_1): LayerScale()\n",
       "        (gamma_2): LayerScale()\n",
       "      )\n",
       "      (1): CrossTransformerEncoderLayer(\n",
       "        (cross_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.02, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_out): MyGroupNorm(1, 512, eps=1e-05, affine=True)\n",
       "        (gamma_1): LayerScale()\n",
       "        (gamma_2): LayerScale()\n",
       "        (dropout1): Dropout(p=0.02, inplace=False)\n",
       "        (dropout2): Dropout(p=0.02, inplace=False)\n",
       "      )\n",
       "      (2): MyTransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.02, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.02, inplace=False)\n",
       "        (dropout2): Dropout(p=0.02, inplace=False)\n",
       "        (norm_out): MyGroupNorm(1, 512, eps=1e-05, affine=True)\n",
       "        (gamma_1): LayerScale()\n",
       "        (gamma_2): LayerScale()\n",
       "      )\n",
       "      (3): CrossTransformerEncoderLayer(\n",
       "        (cross_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.02, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_out): MyGroupNorm(1, 512, eps=1e-05, affine=True)\n",
       "        (gamma_1): LayerScale()\n",
       "        (gamma_2): LayerScale()\n",
       "        (dropout1): Dropout(p=0.02, inplace=False)\n",
       "        (dropout2): Dropout(p=0.02, inplace=False)\n",
       "      )\n",
       "      (4): MyTransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.02, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.02, inplace=False)\n",
       "        (dropout2): Dropout(p=0.02, inplace=False)\n",
       "        (norm_out): MyGroupNorm(1, 512, eps=1e-05, affine=True)\n",
       "        (gamma_1): LayerScale()\n",
       "        (gamma_2): LayerScale()\n",
       "      )\n",
       "    )\n",
       "    (layers_t): ModuleList(\n",
       "      (0): MyTransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.02, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.02, inplace=False)\n",
       "        (dropout2): Dropout(p=0.02, inplace=False)\n",
       "        (norm_out): MyGroupNorm(1, 512, eps=1e-05, affine=True)\n",
       "        (gamma_1): LayerScale()\n",
       "        (gamma_2): LayerScale()\n",
       "      )\n",
       "      (1): CrossTransformerEncoderLayer(\n",
       "        (cross_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.02, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_out): MyGroupNorm(1, 512, eps=1e-05, affine=True)\n",
       "        (gamma_1): LayerScale()\n",
       "        (gamma_2): LayerScale()\n",
       "        (dropout1): Dropout(p=0.02, inplace=False)\n",
       "        (dropout2): Dropout(p=0.02, inplace=False)\n",
       "      )\n",
       "      (2): MyTransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.02, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.02, inplace=False)\n",
       "        (dropout2): Dropout(p=0.02, inplace=False)\n",
       "        (norm_out): MyGroupNorm(1, 512, eps=1e-05, affine=True)\n",
       "        (gamma_1): LayerScale()\n",
       "        (gamma_2): LayerScale()\n",
       "      )\n",
       "      (3): CrossTransformerEncoderLayer(\n",
       "        (cross_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.02, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_out): MyGroupNorm(1, 512, eps=1e-05, affine=True)\n",
       "        (gamma_1): LayerScale()\n",
       "        (gamma_2): LayerScale()\n",
       "        (dropout1): Dropout(p=0.02, inplace=False)\n",
       "        (dropout2): Dropout(p=0.02, inplace=False)\n",
       "      )\n",
       "      (4): MyTransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.02, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.02, inplace=False)\n",
       "        (dropout2): Dropout(p=0.02, inplace=False)\n",
       "        (norm_out): MyGroupNorm(1, 512, eps=1e-05, affine=True)\n",
       "        (gamma_1): LayerScale()\n",
       "        (gamma_2): LayerScale()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "htdemucs = pretrained.get_model('htdemucs') # load pretrained htdemucs\n",
    "\n",
    "# modify network to have 3 stems output\n",
    "model = htdemucs.models[0]\n",
    "model.sources = ['speech', 'music', 'sfx']\n",
    "model.decoder[-1].conv_tr = torch.nn.ConvTranspose2d(\n",
    "    in_channels=48,  \n",
    "    out_channels=12,  # 3 stems * input channels (2 for stereo)\n",
    "    kernel_size=(8, 1),\n",
    "    stride=(4, 1)\n",
    ")\n",
    "model.tdecoder[-1].conv_tr = torch.nn.ConvTranspose1d(\n",
    "    in_channels=48,  \n",
    "    out_channels=6,\n",
    "    kernel_size=8,\n",
    "    stride=4\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset\n",
    "importlib.reload(dataset)\n",
    "\n",
    "# Initialize dataset\n",
    "dataset_path = \"DnR/dnr_small\"  # Use small dataset for local testing. Replace with the full dataset \n",
    "train_path = os.path.join(dataset_path, \"tr\")  # Folder containing training data\n",
    "val_path = os.path.join(dataset_path, \"cv\")  # Folder containing validation data\n",
    "test_path = os.path.join(dataset_path, \"tt\")  # Folder containing test data\n",
    "\n",
    "# Initialize datasets\n",
    "chunk_size = 264600\n",
    "train_dataset = dataset.DnR_Dataset(root_dir=train_path, sample_rate=44100, chunk_size=chunk_size)\n",
    "val_dataset = dataset.DnR_Dataset(root_dir=val_path, sample_rate=44100, chunk_size=chunk_size)\n",
    "test_dataset = dataset.DnR_Dataset(root_dir=test_path, sample_rate=44100, chunk_size=chunk_size)\n",
    "\n",
    "# Initialize DataLoaders\n",
    "batch_size = 8 # update batch size when using GPU\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yt/.pyenv/shims/versions/dl4cv_final/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/yt/.pyenv/shims/versions/dl4cv_final/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [03:49<00:00, 76.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.00049921\n",
      "Val Loss: 0.00059880\n",
      "Validation loss improved to 0.00059880, model saved.\n",
      "--------------------\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [04:05<00:00, 81.79s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.00044242\n",
      "Val Loss: 0.00039084\n",
      "Validation loss improved to 0.00039084, model saved.\n",
      "--------------------\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [04:05<00:00, 81.82s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.00045285\n",
      "Val Loss: 0.00045975\n",
      "No improvement in validation loss. Counter: 1/10\n",
      "--------------------\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [04:04<00:00, 81.61s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.00043116\n",
      "Val Loss: 0.00043526\n",
      "No improvement in validation loss. Counter: 2/10\n",
      "--------------------\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [04:00<00:00, 80.14s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.00048843\n",
      "Val Loss: 0.00048453\n",
      "No improvement in validation loss. Counter: 3/10\n",
      "--------------------\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [03:48<00:00, 76.27s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.00051201\n",
      "Val Loss: 0.00037891\n",
      "Validation loss improved to 0.00037891, model saved.\n",
      "--------------------\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [03:42<00:00, 74.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.00048181\n",
      "Val Loss: 0.00044804\n",
      "No improvement in validation loss. Counter: 1/10\n",
      "--------------------\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [04:20<00:00, 86.75s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.00044949\n",
      "Val Loss: 0.00032283\n",
      "Validation loss improved to 0.00032283, model saved.\n",
      "--------------------\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [04:02<00:00, 80.80s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.00050985\n",
      "Val Loss: 0.00044025\n",
      "No improvement in validation loss. Counter: 1/10\n",
      "--------------------\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [04:11<00:00, 83.99s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.00051913\n",
      "Val Loss: 0.00032449\n",
      "No improvement in validation loss. Counter: 2/10\n",
      "--------------------\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [03:53<00:00, 77.90s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.00058113\n",
      "Val Loss: 0.00050744\n",
      "No improvement in validation loss. Counter: 3/10\n",
      "--------------------\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [03:51<00:00, 77.03s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.00044748\n",
      "Val Loss: 0.00037881\n",
      "No improvement in validation loss. Counter: 4/10\n",
      "--------------------\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [03:55<00:00, 78.42s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.00048301\n",
      "Val Loss: 0.00049471\n",
      "No improvement in validation loss. Counter: 5/10\n",
      "--------------------\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [03:54<00:00, 78.26s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.00043522\n",
      "Val Loss: 0.00045775\n",
      "No improvement in validation loss. Counter: 6/10\n",
      "--------------------\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [04:06<00:00, 82.06s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.00052551\n",
      "Val Loss: 0.00038158\n",
      "No improvement in validation loss. Counter: 7/10\n",
      "--------------------\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [04:05<00:00, 81.77s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.00053632\n",
      "Val Loss: 0.00046285\n",
      "No improvement in validation loss. Counter: 8/10\n",
      "--------------------\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [04:06<00:00, 82.10s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.00042774\n",
      "Val Loss: 0.00079286\n",
      "No improvement in validation loss. Counter: 9/10\n",
      "--------------------\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [04:04<00:00, 81.42s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.00048805\n",
      "Val Loss: 0.00045458\n",
      "No improvement in validation loss. Counter: 10/10\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "import train \n",
    "importlib.reload(train)\n",
    "\n",
    "# Create output directory to save model\n",
    "output_dir = \"./models/local_train\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "model.to(DEVICE)\n",
    "# Train the model\n",
    "trained_model = train.train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    early_stopping = train.EarlyStopping(),\n",
    "    epochs=100,  \n",
    "    learning_rate=1e-4,  # Learning rate\n",
    "    output_dir=output_dir,\n",
    "    device = DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2, 2646000)\n"
     ]
    }
   ],
   "source": [
    "# validate model output\n",
    "def load_audio(audio_path, sample_rate=44100, device='cpu'):\n",
    "    # Load audio with librosa\n",
    "    audio, sr = librosa.load(audio_path, mono=False, sr=sample_rate)\n",
    "\n",
    "    # Handle mono audio by duplicating the channel\n",
    "    if len(audio.shape) == 1:  # Mono audio\n",
    "        audio = np.stack([audio, audio], axis=0)  # Convert to stereo\n",
    "\n",
    "    audio = np.expand_dims(audio, axis=0)  # Shape: [1, samples, channels]\n",
    "    audio = torch.from_numpy(audio).float().to(device)\n",
    "\n",
    "    return audio\n",
    "audio_tensor = load_audio(\"DnR/dnr_small/tr/106/mix.wav\") # change to correct path\n",
    "out = apply_model(trained_model, audio_tensor, shifts=1, overlap=0.8)[0].cpu().numpy()\n",
    "print(out.shape)\n",
    "\n",
    "for i, source in enumerate(out[:,:,:]):\n",
    "    source = np.array(source).mean(axis=0)  # Convert tensor to numpy array\n",
    "    sf.write(f'small_train_masked_loss{i}.wav', source.T, 44100)  # Transpose if needed for correct shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean SDR: -0.19516621530056\n",
      "Mean si_SDR: -14.309621810913086\n"
     ]
    }
   ],
   "source": [
    "# reference: https://github.com/ZFTurbo/Music-Source-Separation-Training/blob/730d162b2ef31a1bba5d4a7d40ae914459a011b8/utils.py#L221\n",
    "# TODO: move evaluation to separate module\n",
    "def sdr(references, estimates):\n",
    "    # compute SDR for one song\n",
    "    delta = 1e-7  # avoid numerical errors\n",
    "    num = np.sum(np.square(references), axis=(1, 2))\n",
    "    den = np.sum(np.square(references - estimates), axis=(1, 2))\n",
    "    num += delta\n",
    "    den += delta\n",
    "    return 10 * np.log10(num / den)\n",
    "\n",
    "def si_sdr(reference, estimate):\n",
    "    eps = 1e-07\n",
    "    scale = np.sum(estimate * reference + eps, axis=(0, 1)) / np.sum(reference**2 + eps, axis=(0, 1))\n",
    "    scale = np.expand_dims(scale, axis=(0, 1))  # shape - [50, 1]\n",
    "    reference = reference * scale\n",
    "    sisdr = np.mean(10 * np.log10(np.sum(reference**2, axis=(0, 1)) / (np.sum((reference - estimate)**2, axis=(0, 1)) + eps) + eps))\n",
    "    return sisdr\n",
    "\n",
    "references_path_prefix = \"DnR/dnr_small/tr/106\"\n",
    "predicted_file_prefix = \"small_train_masked_loss\"\n",
    "references = []\n",
    "predicted = []\n",
    "for i, source in enumerate(model.sources):\n",
    "    references += load_audio(f\"{references_path_prefix}/{source}.wav\")\n",
    "    predicted += load_audio(f\"{predicted_file_prefix}{i}.wav\")\n",
    "\n",
    "references = np.stack(references)  # Shape: [num_sources, channels, samples]\n",
    "predicted = np.stack(predicted)  # Shape: [num_sources, channels, samples]\n",
    "\n",
    "# Compute SDR\n",
    "mean_sdr = np.mean(sdr(references, predicted))\n",
    "print(f\"Mean SDR: {mean_sdr}\")\n",
    "\n",
    "si_mean_sdr = np.mean(si_sdr(references, predicted))\n",
    "print(f\"Mean si_SDR: {si_mean_sdr}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl4cv_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
